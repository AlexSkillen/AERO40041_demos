{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b79f959-f935-49f8-9d8d-49ad9db5ccfa",
   "metadata": {},
   "source": [
    "# Logistic regression; Comparison between MSE and binary cross-entropy\n",
    "\n",
    "This webpage explores both binary cross-entropy loss and MSE loss for logistic regression. Our model can be diagrammatically depicted as follows (for a single feature):\n",
    "\n",
    "<img src=\"https://alexskillen.github.io/AERO40041/images/Logistic_regression/LogisticRegression.png\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "The feature, $x$, is multiplied by weight $w$, and added to bias $b$ to give the activation potential $n$. This is then passed into the sigmoid function (denoted $\\sigma$) to give the output $a$ (often denoted as $\\hat{y}$). The model form it therefore $\\hat{y} = \\sigma(wx+b)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e1948f-bc9f-45f9-ae23-6bde6f88dc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from ipywidgets import interact\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab637e8-974b-4cd8-81a0-952a1d75a802",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    if( z>=0. ):\n",
    "        return 1. / (1. + np.exp(-z))\n",
    "    else:\n",
    "        return np.exp(z) / (1. + np.exp(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d317f5-7809-4258-8075-8102f859a27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_dx_sigmoid(z):\n",
    "    return sigmoid(z)*(1.-sigmoid(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c6e2f6-4778-4ce5-8a5f-b16b0dadb67d",
   "metadata": {},
   "source": [
    "The sigmoid function is given by\n",
    "$$\\sigma(z) = \\frac{1}{1+e^{-z}}.$$\n",
    "\n",
    "Its derivative is given by \n",
    "$$\\sigma^\\prime(z) = \\sigma(z) (1-\\sigma(z)).$$\n",
    "\n",
    "We plot the sigmoid function and its derivative below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3323efe2-f734-439a-9005-5859d7bf3872",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.linspace(-20, 20, 100)\n",
    "\n",
    "sigz = [sigmoid(x) for x in z]\n",
    "dsigz = [d_dx_sigmoid(x) for x in z]\n",
    "\n",
    "plt.plot(z, sigz, label=r'$\\sigma(z)$')\n",
    "plt.plot(z, dsigz, label=r'$\\frac{\\partial \\sigma(z)}{\\partial x}$')\n",
    "plt.legend()\n",
    "plt.xlabel(r'$z$')\n",
    "plt.ylabel(r'$\\sigma(z), \\sigma^\\prime(z)$')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccec6531-d697-490a-8e39-6919b6f69d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSEloss(x, y, w, b):\n",
    "    return (y - sigmoid(w*x+b))**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe32a301-9860-4f56-a767-b2ebb68956ce",
   "metadata": {},
   "source": [
    "We will consider a toy problem with one data sample; $x=1, y=0$. From the plot above, it is apparent that the solution to this is to make the input to the sigmoid function, $n=wx+b$, a large negative value (note the sigmoid function asymptotes towards zero at negative values of its input, so the more negative $n$ is, the better). This can be achieved by making both $b$ and $w$ large negative values.\n",
    "\n",
    "We will train the model with gradient descent. To do so, we need the gradient of the loss function with respect to the parameters. Using the chain rule, this can be expressed as the product of three partial derivatives, each of which can be found easily:\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial w} = \\frac{\\partial \\mathcal{L}}{\\partial a}\\frac{\\partial a}{\\partial n}\\frac{\\partial n}{\\partial w}$$\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial b} = \\frac{\\partial \\mathcal{L}}{\\partial a}\\frac{\\partial a}{\\partial n}\\frac{\\partial n}{\\partial b}$$\n",
    "\n",
    "If using a MSE loss, the partial derivative $\\partial \\mathcal{L} / \\partial a$ can be expressed as \n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial a} = -2(y-\\hat{y})$$\n",
    "\n",
    "where $y$ is the label (zero in this case), and $\\hat{y} ~ (\\equiv a)$ is the model's current prediction. \n",
    "\n",
    "The partial derivative $\\partial a / \\partial n$ is the gradient of the sigmoid function at $n$, i.e. \n",
    "\n",
    "$$\\frac{\\partial a}{\\partial n} = \\sigma^\\prime(n)$$.\n",
    "\n",
    "Finally, since $n=wx+b$ the derivatives $\\partial n / \\partial w$ and $\\partial n / \\partial b$ are equal to $x$ and $1$, respectively (since we have only one data point)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb39b0b-b321-4a43-b035-9134dcfda95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_wb( x, y, w, b, alpha=0.1 ):\n",
    "    a = sigmoid(w*x+b)\n",
    "    dl_da = -2.*(y - a)\n",
    "    da_dn = d_dx_sigmoid(w*x+b)\n",
    "    dn_dw = x\n",
    "\n",
    "    dn_db = 1.\n",
    "\n",
    "    w = w - alpha * (dl_da*da_dn*dn_dw)\n",
    "    b = b - alpha * (dl_da*da_dn*dn_db)\n",
    "\n",
    "    return w, b, dl_da, da_dn, dn_dw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e45ab0-30cf-4fc5-bf2b-286d8bd30640",
   "metadata": {},
   "source": [
    "In the plots below, we show how the MSE loss changes with the evolution of the parameters $w$ and $b$ updated by gradient descent with a fixed learning rate of $0.1$. The initial values of $w$ and $b$ can be controlled with the sliders. \n",
    "\n",
    "The bottom row of plots shows the individual gradient terms $\\partial \\mathcal{L} / \\partial a$, $\\partial a / \\partial n$ and $\\partial n / \\partial w$. Their product gives the gradient of the loss with respect to the parameter $w$, and similarly for $b$ (since in this particular case, $x=1$, the update for $b$ will be identical to that of $w$.). \n",
    "\n",
    "Try setting the initial values of $w$ and $b$ to $0.2$. We can see that the model predicts the value of $0.075$ after 300 epochs. The true output is zero, so this is not too bad, but there is certainly room for improvement. \n",
    "\n",
    "Now try changing the initial values of the parameters to $2$. The model output after $300$ epochs is $0.12$; quite far from the target of zero. We can see from the loss plot that learning is very slow for the first 100 or so epochs. Have a think about why this might be and answer below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae8a7271-d3ac-4b8f-8e38-a0b49957fb59",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.\u001b[39m\n\u001b[1;32m      2\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.\u001b[39m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mstyle\u001b[38;5;241m.\u001b[39muse(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseaborn\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m initial_w \u001b[38;5;241m=\u001b[39m widgets\u001b[38;5;241m.\u001b[39mFloatSlider(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5.0\u001b[39m, description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInitial w:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m initial_b \u001b[38;5;241m=\u001b[39m widgets\u001b[38;5;241m.\u001b[39mFloatSlider(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5.0\u001b[39m, description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInitial b:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "x = 1.\n",
    "y = 0.\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "initial_w = widgets.FloatSlider(2, min=-5, max=5.0, description='Initial w:')\n",
    "initial_b = widgets.FloatSlider(2, min=-5, max=5.0, description='Initial b:')\n",
    "sub = widgets.Button(description='Submit')\n",
    "out = widgets.Output()\n",
    "\n",
    "display(initial_w)\n",
    "display(initial_b)\n",
    "display(sub, out)\n",
    "\n",
    "def train(b):\n",
    "    clear_outputs()\n",
    "    with out:\n",
    "        epochs=300\n",
    "        w=initial_w.value\n",
    "        b=initial_b.value\n",
    "        L_history = []\n",
    "        w_history = []\n",
    "        b_history = []\n",
    "        dlda_history = []\n",
    "        dadn_history = []\n",
    "        dndw_history = []\n",
    "    \n",
    "        for _ in range(int(epochs)):\n",
    "            loss = MSEloss( x, y, w, b )\n",
    "            L_history.append(loss)\n",
    "            w_history.append(w)\n",
    "            b_history.append(b)\n",
    "        \n",
    "            w, b, dl_da, da_dn, dn_dw = update_wb( x, y, w, b )\n",
    "\n",
    "            dlda_history.append(dl_da)\n",
    "            dadn_history.append(da_dn)\n",
    "            dndw_history.append(dn_dw)\n",
    "        \n",
    "        fig, ax = plt.subplots(2, 3, figsize=(15,10))\n",
    "        ax[0][0].plot(L_history)\n",
    "        ax[0][1].plot(w_history)\n",
    "        ax[0][2].plot(b_history)\n",
    "        ax[1][0].plot(dlda_history)\n",
    "        ax[1][1].plot(dadn_history)\n",
    "        ax[1][2].plot(dndw_history)\n",
    "\n",
    "        ax[0][0].set_xlabel('epoch')\n",
    "        ax[0][1].set_xlabel('epoch')\n",
    "        ax[0][2].set_xlabel('epoch')\n",
    "        ax[1][0].set_xlabel('epoch')\n",
    "        ax[1][1].set_xlabel('epoch')\n",
    "        ax[1][2].set_xlabel('epoch')\n",
    "\n",
    "        ax[0][0].set_ylabel('MSE loss', fontsize=18)\n",
    "        ax[0][1].set_ylabel('w', fontsize=18)\n",
    "        ax[0][2].set_ylabel('b', fontsize=18)\n",
    "        ax[1][0].set_ylabel(r'∂L/∂a', fontsize=18)\n",
    "        ax[1][1].set_ylabel(r'∂a/∂n', fontsize=18)\n",
    "        ax[1][2].set_ylabel(r'∂n/∂w', fontsize=18)\n",
    "\n",
    "        print( \"The Final outout:\", sigmoid(w*x+b) )\n",
    "        plt.show()\n",
    "\n",
    "def clear_outputs():\n",
    "    out.clear_output(wait=True)\n",
    "\n",
    "sub.on_click(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5232a3-f2fc-435b-a197-f1d0043cd9d4",
   "metadata": {},
   "source": [
    "example=[{\n",
    "        \"question\": \"Why is the learning initially so slow?\",\n",
    "        \"type\": \"multiple_choice\",\n",
    "        \"answers\": [\n",
    "            {\n",
    "                \"answer\": r'∂L/∂a is small',\n",
    "                \"correct\": False,\n",
    "                \"feedback\": \".\"\n",
    "            },\n",
    "            {\n",
    "                \"answer\": r'∂a/∂n is small',\n",
    "                \"correct\": True,\n",
    "                \"feedback\": r'Yes! Remember da/dn = the gradient of the sigmoid function. Look at the plot above. If we are very far from the true answer the gradient of the sigmoid function will be near zero.'\n",
    "            }\n",
    "        ]\n",
    "    }]\n",
    "\n",
    "from jupyterquiz import display_quiz\n",
    "display_quiz(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf66dcd8-5ac0-484f-a3f0-4bdd04dfb2df",
   "metadata": {},
   "source": [
    "Now let's compare the binary cross-entropy loss, or Log loss. We prepare the same set of plots as before, but now the loss function is given by \n",
    "\n",
    "$$\\mathrm{Log Loss} = \\sum_{i=1}^N -y_i \\ln(\\hat{y}) - (1-y_i)\\ln(1-\\hat{y}).$$\n",
    "\n",
    "In this toy problem, since $y$ is always zero, this reduces to \n",
    "\n",
    "$$\\mathrm{Log Loss} = - \\ln(1-\\hat{y}) ~~~ \\mathrm{or} ~~~ \\mathcal{L} = - \\ln(1-a).$$\n",
    "\n",
    "The partial derivative $\\partial \\mathcal{L} / \\partial a$ is now given by $1/(1-a)$ for this particular case (with $y=0$). \n",
    "\n",
    "Initialise the parameters both to $2$ and we observe an output of $0.018$ after $300$ epochs with the same learning rate of $0.1$. This is much closer to the target value than that found with MSE! Have a think why, and answer below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370618df-a72c-4284-9d9b-24b71b929de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CEloss(x, y, w, b):\n",
    "    return -(1.)*np.log(1.-sigmoid(w*x+b)+1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3aab6c6-1e2c-47e6-a620-e0d313bf1894",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_wb_CE( x, y, w, b, alpha=0.1 ):\n",
    "    a = sigmoid(w*x+b)\n",
    "    dl_da = (1.)/(1.-a) \n",
    "    da_dn = d_dx_sigmoid(w*x+b)\n",
    "    dn_dw = x\n",
    "\n",
    "    dn_db = 1.\n",
    "\n",
    "    w = w - alpha * (dl_da*da_dn*dn_dw)\n",
    "    b = b - alpha * (dl_da*da_dn*dn_db)\n",
    "\n",
    "    return w, b, dl_da, da_dn, dn_dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79b4de4-b2ca-4f4a-aca9-b20ce60b6f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 1.\n",
    "y = 0.\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "initial_w2 = widgets.FloatSlider(2, min=-5, max=5.0, description='Initial w:')\n",
    "initial_b2 = widgets.FloatSlider(2, min=-5, max=5.0, description='Initial b:')\n",
    "sub2 = widgets.Button(description='Submit')\n",
    "out2 = widgets.Output()\n",
    "\n",
    "display(initial_w2)\n",
    "display(initial_b2)\n",
    "display(sub2, out2)\n",
    "\n",
    "def train(b):\n",
    "    clear_outputs2()\n",
    "    with out2:\n",
    "        epochs=300\n",
    "        w=initial_w2.value\n",
    "        b=initial_b2.value\n",
    "        L_history = []\n",
    "        w_history = []\n",
    "        b_history = []\n",
    "        dlda_history = []\n",
    "        dadn_history = []\n",
    "        dndw_history = []\n",
    "    \n",
    "        for _ in range(int(epochs)):\n",
    "            loss = CEloss( x, y, w, b )\n",
    "            L_history.append(loss)\n",
    "            w_history.append(w)\n",
    "            b_history.append(b)\n",
    "        \n",
    "            w, b, dl_da, da_dn, dn_dw = update_wb_CE( x, y, w, b )\n",
    "\n",
    "            dlda_history.append(dl_da)\n",
    "            dadn_history.append(da_dn)\n",
    "            dndw_history.append(dn_dw)\n",
    "        \n",
    "        fig, ax = plt.subplots(2, 3, figsize=(15,10))\n",
    "        ax[0][0].plot(L_history)\n",
    "        ax[0][1].plot(w_history)\n",
    "        ax[0][2].plot(b_history)\n",
    "        ax[1][0].plot(dlda_history)\n",
    "        ax[1][1].plot(dadn_history)\n",
    "        ax[1][2].plot(dndw_history)\n",
    "\n",
    "        ax[0][0].set_xlabel('epoch')\n",
    "        ax[0][1].set_xlabel('epoch')\n",
    "        ax[0][2].set_xlabel('epoch')\n",
    "        ax[1][0].set_xlabel('epoch')\n",
    "        ax[1][1].set_xlabel('epoch')\n",
    "        ax[1][2].set_xlabel('epoch')\n",
    "\n",
    "        ax[0][0].set_ylabel('CE loss', fontsize=18)\n",
    "        ax[0][1].set_ylabel('w', fontsize=18)\n",
    "        ax[0][2].set_ylabel('b', fontsize=18)\n",
    "        ax[1][0].set_ylabel(r'∂L/∂a', fontsize=18)\n",
    "        ax[1][1].set_ylabel(r'∂a/∂n', fontsize=18)\n",
    "        ax[1][2].set_ylabel(r'∂n/∂w', fontsize=18)\n",
    "\n",
    "        print( \"The Final outout:\", sigmoid(w*x+b) )\n",
    "        plt.show()\n",
    "\n",
    "def clear_outputs2():\n",
    "    out2.clear_output(wait=True)\n",
    "\n",
    "sub2.on_click(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb4763f-032e-490d-a3dd-1951b59bc4ce",
   "metadata": {},
   "source": [
    "example=[{\n",
    "        \"question\": \"Why is the initial learning now so much better?\",\n",
    "        \"type\": \"multiple_choice\",\n",
    "        \"answers\": [\n",
    "            {\n",
    "                \"answer\": r'$\\partial \\mathcal{L} / \\partial a$ is large',\n",
    "                \"correct\": True,\n",
    "                \"feedback\": r'Yes! Even though da/dn remains small, dL/da will be large. In this toy problem, we have  dL/da = 1/(1-a). If a is close to 1 (i.e. a very bad prediction far from the target of zero), the denominator will be small, and the derivative will therefore be large. When multiplied by the small value of the gradient of the sigmoid their product will be sufficiently large for effective learning.'\n",
    "            },\n",
    "            {\n",
    "                \"answer\": r'$\\partial \\mathcal{a} / \\partial n$ is large',\n",
    "                \"correct\": False,\n",
    "                \"feedback\": r'No. Remember da/dn = the gradient of the sigmoid function. Look at the plot above. If we are very far from the true answer the gradient of the sigmoid function will be near zero.'\n",
    "            }\n",
    "        ]\n",
    "    }]\n",
    "\n",
    "from jupyterquiz import display_quiz\n",
    "display_quiz(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99007ed-411b-406b-b103-47b9a3d41e4b",
   "metadata": {},
   "source": [
    "Even though this is a toy problem, the same conclusions hold for more complex data and in higher dimensional space. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
